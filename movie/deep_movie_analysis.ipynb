{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 12/04/2018\n",
    "    Description \n",
    "        - Basic deep learning with Pytorch\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, vocab\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load training/testing/word2vec data\n",
    "'''\n",
    "\n",
    "root_path = './data/aclImdb/'\n",
    "\n",
    "dict_path = './data/word2vec.txt'\n",
    "\n",
    "vec = vocab.Vectors(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Prepare dataset using torchtext\n",
    "'''\n",
    "LABEL = data.Field(unk_token=None, pad_token=None, sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(batch_first=True)\n",
    "\n",
    "train_set, test_set = data.TabularDataset.splits(path=root_path, \n",
    "                                                  format='csv', \n",
    "                                                  skip_header = True, train = 'train.csv', test='test.csv',\n",
    "                                                  fields=[('label',LABEL),('text',TEXT)])\n",
    "\n",
    "# Further split train_set into train/validation\n",
    "train_set, valid_set = train_set.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields of training/testing dataset:  dict_items([('label', <torchtext.data.field.Field object at 0x3ffd2115cc88>), ('text', <torchtext.data.field.Field object at 0x3ffd2115ccf8>)])\n",
      "Length of training dataset:  17500\n",
      "Length of validation dataset:  7500\n",
      "Length of test dataset:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Fields of training/testing dataset: \", train_set.fields.items())\n",
    "print(\"Length of training dataset: \", len(train_set))\n",
    "print(\"Length of validation dataset: \", len(valid_set))\n",
    "print(\"Length of test dataset: \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': ['according', 'to', 'the', 'director', 'this', 'movie', 'was', 'popular', 'in', 'asia', 'it', 'is', 'somewhat', 'difficult', 'to', 'take', 'these', 'mats', 'helge', 'movies', 'seriously', 'since', 'most', 'of', 'his', 'films', 'are', 'shot', 'on', 'a', 'very', 'tight', 'budget', 'almost', 'no', 'usd', 'at', 'all', 'but', 'it', 'is', 'fascinating', 'to', 'establish', 'that', 'mats', 'helge', 'eventually', 'completes', 'something', 'which', 'can', 'be', 'called', 'an', 'action', 'movie', 'the', 'ninja', 'mission', 'is', 'i', 'think', 'the', 'best', 'one', 'among', 'all', 'movies', 'directed', 'by', 'him', 'some', 'special', 'effects', 'are', 'quite', 'enjoyable', 'this', 'is', 'not', 'a', 'b', 'or', 'c', 'movie', 'it', 'is', 'a', 'z', 'movie', 'but', 'an', 'enjoyable', 'and', 'fun', 'z', 'movie']}\n"
     ]
    }
   ],
   "source": [
    "# Checking a training data\n",
    "print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator\n",
    "data_batch_size = 128\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train_set, valid_set, test_set), batch_size=data_batch_size, \n",
    "                                                   sort_key=lambda x: len(x.text), device=device, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102661, 50])\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "LABEL.build_vocab(train_set)\n",
    "TEXT.build_vocab(train_set, valid_set, test_set, vectors=vec)\n",
    "\n",
    "# Check the vocabulary shape\n",
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102661"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': ['according', 'to', 'the', 'director', 'this', 'movie', 'was', 'popular', 'in', 'asia', 'it', 'is', 'somewhat', 'difficult', 'to', 'take', 'these', 'mats', 'helge', 'movies', 'seriously', 'since', 'most', 'of', 'his', 'films', 'are', 'shot', 'on', 'a', 'very', 'tight', 'budget', 'almost', 'no', 'usd', 'at', 'all', 'but', 'it', 'is', 'fascinating', 'to', 'establish', 'that', 'mats', 'helge', 'eventually', 'completes', 'something', 'which', 'can', 'be', 'called', 'an', 'action', 'movie', 'the', 'ninja', 'mission', 'is', 'i', 'think', 'the', 'best', 'one', 'among', 'all', 'movies', 'directed', 'by', 'him', 'some', 'special', 'effects', 'are', 'quite', 'enjoyable', 'this', 'is', 'not', 'a', 'b', 'or', 'c', 'movie', 'it', 'is', 'a', 'z', 'movie', 'but', 'an', 'enjoyable', 'and', 'fun', 'z', 'movie']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 857])\n"
     ]
    }
   ],
   "source": [
    "# Check the sample batch\n",
    "sample_train_batch = next(iter(train_iter))\n",
    "\n",
    "print(sample_train_batch.text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4315, -2.6598,  0.4538, -0.4569,  1.1227,  0.2325, -0.2207,\n",
       "         2.3917, -2.2225,  1.6792, -0.8594,  1.1664,  0.8578,  0.0004,\n",
       "        -1.2418, -3.3718,  0.9664,  1.0759,  5.0070,  1.2052,  1.0962,\n",
       "         2.0620,  1.5243,  1.4128, -2.3810,  3.3441, -1.6822, -0.2576,\n",
       "        -1.1957,  1.3121,  2.7869,  3.1460,  1.5586,  0.4723,  1.2427,\n",
       "         0.6243, -1.7932, -3.9185,  2.2868, -2.0155,  3.8783, -2.0760,\n",
       "        -1.8423, -0.2244,  3.4017, -2.4928, -2.1677,  0.8344,  3.0949,\n",
       "        -3.1904])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tensor for a sample input word\n",
    "TEXT.vocab.vectors[TEXT.vocab.stoi['a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 667700), ('and', 324294), ('a', 322886), ('of', 289379), ('to', 268071), ('is', 211046), ('it', 190675), ('in', 186718), ('i', 175431), ('this', 150909)]\n"
     ]
    }
   ],
   "source": [
    "# Check the top 10 frequent words in vocabulary\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['according',\n",
       "  'to',\n",
       "  'the',\n",
       "  'director',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'was',\n",
       "  'popular',\n",
       "  'in',\n",
       "  'asia',\n",
       "  'it',\n",
       "  'is',\n",
       "  'somewhat',\n",
       "  'difficult',\n",
       "  'to',\n",
       "  'take',\n",
       "  'these',\n",
       "  'mats',\n",
       "  'helge',\n",
       "  'movies',\n",
       "  'seriously',\n",
       "  'since',\n",
       "  'most',\n",
       "  'of',\n",
       "  'his',\n",
       "  'films',\n",
       "  'are',\n",
       "  'shot',\n",
       "  'on',\n",
       "  'a',\n",
       "  'very',\n",
       "  'tight',\n",
       "  'budget',\n",
       "  'almost',\n",
       "  'no',\n",
       "  'usd',\n",
       "  'at',\n",
       "  'all',\n",
       "  'but',\n",
       "  'it',\n",
       "  'is',\n",
       "  'fascinating',\n",
       "  'to',\n",
       "  'establish',\n",
       "  'that',\n",
       "  'mats',\n",
       "  'helge',\n",
       "  'eventually',\n",
       "  'completes',\n",
       "  'something',\n",
       "  'which',\n",
       "  'can',\n",
       "  'be',\n",
       "  'called',\n",
       "  'an',\n",
       "  'action',\n",
       "  'movie',\n",
       "  'the',\n",
       "  'ninja',\n",
       "  'mission',\n",
       "  'is',\n",
       "  'i',\n",
       "  'think',\n",
       "  'the',\n",
       "  'best',\n",
       "  'one',\n",
       "  'among',\n",
       "  'all',\n",
       "  'movies',\n",
       "  'directed',\n",
       "  'by',\n",
       "  'him',\n",
       "  'some',\n",
       "  'special',\n",
       "  'effects',\n",
       "  'are',\n",
       "  'quite',\n",
       "  'enjoyable',\n",
       "  'this',\n",
       "  'is',\n",
       "  'not',\n",
       "  'a',\n",
       "  'b',\n",
       "  'or',\n",
       "  'c',\n",
       "  'movie',\n",
       "  'it',\n",
       "  'is',\n",
       "  'a',\n",
       "  'z',\n",
       "  'movie',\n",
       "  'but',\n",
       "  'an',\n",
       "  'enjoyable',\n",
       "  'and',\n",
       "  'fun',\n",
       "  'z',\n",
       "  'movie'],\n",
       " '1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = train_set[0]\n",
    "ex.text, ex.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights, vocab_size, embedding_length=50, hidden_size=128, output_size=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = vocab_size\n",
    "        self.embedding_dim  = embedding_length\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.output_dim = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.RNN(input_size=self.embedding_dim, hidden_size=self.hidden_dim)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.zeros(1, bs, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, bs, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, input_word):\n",
    "        # input_word -> batch_size * sent_length = 128* 784\n",
    "        # hidden -> num_layer * batch_size * hidden_dim = 1*128*128\n",
    "        batch_size = input_word.shape[0] # 784\n",
    "        h_0 = self.init_hidden(batch_size) #1*784*128\n",
    "        \n",
    "        embedded = self.embedding(input_word)   # embedded -> batch_size * sent_length * embedding_dim = 128*784*50\n",
    "        \n",
    "        embedded = embedded.permute(1,0,2)      # embedded -> sent_length * batch_size * embedding_dim = 784*128*50\n",
    "        \n",
    "        rnn_out, hidden = self.rnn(embedded, h_0)    # rnn_out -> sent_length * batch_size * hidden_dim\n",
    "                                                # hidden -> num_layer * batch_size * hidden_dim\n",
    "        output = self.fc(hidden[-1])          # output -> batch_size * output_dim\n",
    "                                    \n",
    "        return output    # output -> batch_size * output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_2(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        self.rnn = nn.RNN(embedding_length, hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, batch_size, self.hidden_size))    \n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n",
    "        logits.size() = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        \n",
    "        h_0 = self.init_hidden(input_sentences.shape[0])\n",
    "        \n",
    "        output, h_n = self.rnn(input, h_0)\n",
    "        # h_n.size() = (4, batch_size, hidden_size)\n",
    "        h_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n",
    "        h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "        # h_n.size() = (batch_size, 4*hidden_size)\n",
    "        logits = self.label(h_n) # logits.size() = (batch_size, output_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_length = vec.vectors.shape\n",
    "hidden_dim = 256\n",
    "output_dim = len(LABEL.vocab)\n",
    "\n",
    "model = RNN(weights=vec.vectors, hidden_size=hidden_dim,output_size=output_dim, vocab_size=vocab_size, embedding_length=embedding_length)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = []\n",
    "y_predicted = []\n",
    "total_loss_train = 0\n",
    "\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "def train_model(model=model, train_iter=train_iter, epoch=0):\n",
    "    model.train()\n",
    "    \n",
    "    # After epoch\n",
    "    y_gt = []\n",
    "    y_predicted = []\n",
    "    steps = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for index, (X,y) in enumerate(train_iter):\n",
    "        target = X[0]\n",
    "        target = target.view(-1)\n",
    "        input_word = X[1]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "            input_word = input_word.cuda()\n",
    "\n",
    "#         print(input_word.shape)\n",
    "        # Forward pass\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(input_word)\n",
    "#         print(output.shape)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_gradient(model, 1e-1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_idx = torch.max(output, dim=1)[1]\n",
    "\n",
    "        y_gt += list(target.cpu().data.numpy())\n",
    "        y_predicted += list(pred_idx.cpu().data.numpy())\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        acc = accuracy_score(y_gt, y_predicted)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 40 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {index+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "    train_acc = accuracy_score(y_gt, y_predicted)\n",
    "    train_loss = total_loss_train/len(train_iter)\n",
    "    \n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model=model, train_iter=val_iter, epoch=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # After epoch\n",
    "    y_gt = []\n",
    "    y_predicted = []\n",
    "    steps = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for index, (X,y) in enumerate(train_iter):\n",
    "        target = X[0]\n",
    "        target = target.view(-1)\n",
    "        input_word = X[1]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "            input_word = input_word.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_word)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        pred_idx = torch.max(output, dim=1)[1]\n",
    "\n",
    "        y_gt += list(target.cpu().data.numpy())\n",
    "        y_predicted += list(pred_idx.cpu().data.numpy())\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        acc = accuracy_score(y_gt, y_predicted)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 40 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {index+1}, Validation Loss: {loss.item():.4f}, Validation Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "    train_acc = accuracy_score(y_gt, y_predicted)\n",
    "    train_loss = total_loss_train/len(val_iter)\n",
    "    \n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 40, Training Loss: 0.6920, Training Accuracy:  0.49%\n",
      "Epoch: 1, Idx: 80, Training Loss: 0.6921, Training Accuracy:  0.50%\n",
      "Epoch: 1, Idx: 120, Training Loss: 0.6935, Training Accuracy:  0.51%\n",
      "Epoch: 1, Idx: 40, Validation Loss: 0.6968, Validation Accuracy:  0.50%\n",
      "Epoch: 1, Training Accuracy: 0.5075%, Training Loss: 0.6935, Validation Accuracy: 0.5025%, Validation Loss: 0.6978\n",
      "Epoch: 2, Idx: 40, Training Loss: 0.6966, Training Accuracy:  0.51%\n",
      "Epoch: 2, Idx: 80, Training Loss: 0.6929, Training Accuracy:  0.51%\n",
      "Epoch: 2, Idx: 120, Training Loss: 0.6915, Training Accuracy:  0.50%\n",
      "Epoch: 2, Idx: 40, Validation Loss: 0.6950, Validation Accuracy:  0.50%\n",
      "Epoch: 2, Training Accuracy: 0.5013%, Training Loss: 0.6934, Validation Accuracy: 0.5024%, Validation Loss: 0.6971\n",
      "Epoch: 3, Idx: 40, Training Loss: 0.6919, Training Accuracy:  0.48%\n",
      "Epoch: 3, Idx: 80, Training Loss: 0.6943, Training Accuracy:  0.49%\n",
      "Epoch: 3, Idx: 120, Training Loss: 0.6918, Training Accuracy:  0.50%\n",
      "Epoch: 3, Idx: 40, Validation Loss: 0.6943, Validation Accuracy:  0.50%\n",
      "Epoch: 3, Training Accuracy: 0.4977%, Training Loss: 0.6935, Validation Accuracy: 0.4940%, Validation Loss: 0.6971\n",
      "Epoch: 4, Idx: 40, Training Loss: 0.6912, Training Accuracy:  0.50%\n",
      "Epoch: 4, Idx: 80, Training Loss: 0.6960, Training Accuracy:  0.50%\n",
      "Epoch: 4, Idx: 120, Training Loss: 0.6931, Training Accuracy:  0.50%\n",
      "Epoch: 4, Idx: 40, Validation Loss: 0.6941, Validation Accuracy:  0.50%\n",
      "Epoch: 4, Training Accuracy: 0.4967%, Training Loss: 0.6936, Validation Accuracy: 0.4921%, Validation Loss: 0.6969\n",
      "Epoch: 5, Idx: 40, Training Loss: 0.6926, Training Accuracy:  0.50%\n",
      "Epoch: 5, Idx: 80, Training Loss: 0.6980, Training Accuracy:  0.50%\n",
      "Epoch: 5, Idx: 120, Training Loss: 0.6963, Training Accuracy:  0.50%\n",
      "Epoch: 5, Idx: 40, Validation Loss: 0.6945, Validation Accuracy:  0.49%\n",
      "Epoch: 5, Training Accuracy: 0.4993%, Training Loss: 0.6936, Validation Accuracy: 0.5016%, Validation Loss: 0.6971\n",
      "Epoch: 6, Idx: 40, Training Loss: 0.6932, Training Accuracy:  0.50%\n",
      "Epoch: 6, Idx: 80, Training Loss: 0.6937, Training Accuracy:  0.49%\n",
      "Epoch: 6, Idx: 120, Training Loss: 0.6935, Training Accuracy:  0.50%\n",
      "Epoch: 6, Idx: 40, Validation Loss: 0.6943, Validation Accuracy:  0.49%\n",
      "Epoch: 6, Training Accuracy: 0.4965%, Training Loss: 0.6933, Validation Accuracy: 0.5001%, Validation Loss: 0.6970\n",
      "Epoch: 7, Idx: 40, Training Loss: 0.6940, Training Accuracy:  0.50%\n",
      "Epoch: 7, Idx: 80, Training Loss: 0.6945, Training Accuracy:  0.50%\n",
      "Epoch: 7, Idx: 120, Training Loss: 0.6920, Training Accuracy:  0.50%\n",
      "Epoch: 7, Idx: 40, Validation Loss: 0.6927, Validation Accuracy:  0.50%\n",
      "Epoch: 7, Training Accuracy: 0.5011%, Training Loss: 0.6933, Validation Accuracy: 0.4972%, Validation Loss: 0.6973\n",
      "Epoch: 8, Idx: 40, Training Loss: 0.6956, Training Accuracy:  0.50%\n",
      "Epoch: 8, Idx: 80, Training Loss: 0.6912, Training Accuracy:  0.50%\n",
      "Epoch: 8, Idx: 120, Training Loss: 0.6876, Training Accuracy:  0.50%\n",
      "Epoch: 8, Idx: 40, Validation Loss: 0.6927, Validation Accuracy:  0.49%\n",
      "Epoch: 8, Training Accuracy: 0.5031%, Training Loss: 0.6938, Validation Accuracy: 0.4928%, Validation Loss: 0.6967\n",
      "Epoch: 9, Idx: 40, Training Loss: 0.6922, Training Accuracy:  0.49%\n",
      "Epoch: 9, Idx: 80, Training Loss: 0.6951, Training Accuracy:  0.50%\n",
      "Epoch: 9, Idx: 120, Training Loss: 0.6935, Training Accuracy:  0.50%\n",
      "Epoch: 9, Idx: 40, Validation Loss: 0.6927, Validation Accuracy:  0.50%\n",
      "Epoch: 9, Training Accuracy: 0.4971%, Training Loss: 0.6934, Validation Accuracy: 0.4956%, Validation Loss: 0.6968\n",
      "Epoch: 10, Idx: 40, Training Loss: 0.6935, Training Accuracy:  0.50%\n",
      "Epoch: 10, Idx: 80, Training Loss: 0.6937, Training Accuracy:  0.50%\n",
      "Epoch: 10, Idx: 120, Training Loss: 0.6919, Training Accuracy:  0.50%\n",
      "Epoch: 10, Idx: 40, Validation Loss: 0.6957, Validation Accuracy:  0.49%\n",
      "Epoch: 10, Training Accuracy: 0.4993%, Training Loss: 0.6933, Validation Accuracy: 0.5011%, Validation Loss: 0.6980\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    train_acc, train_loss = train_model(model, train_iter, epoch)\n",
    "    val_acc, val_loss = eval_model(model, val_iter, epoch)\n",
    "    \n",
    "    print (f'Epoch: {epoch+1}, Training Accuracy: {train_acc:.4f}%, Training Loss: {train_loss:.4f}, Validation Accuracy: {val_acc:.4f}%, Validation Loss: {val_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
