{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 12/04/2018\n",
    "    Description \n",
    "        - Basic deep learning with Pytorch\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data, vocab\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load training/testing/word2vec data\n",
    "'''\n",
    "\n",
    "root_path = './data/aclImdb/'\n",
    "\n",
    "dict_path = './data/word2vec.txt'\n",
    "\n",
    "vec = vocab.Vectors(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Prepare dataset using torchtext\n",
    "'''\n",
    "LABEL = data.Field(unk_token=None, pad_token=None, sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(batch_first=True)\n",
    "\n",
    "train_set, test_set = data.TabularDataset.splits(path=root_path, \n",
    "                                                  format='csv', \n",
    "                                                  skip_header = True, train = 'train.csv', test='test.csv',\n",
    "                                                  fields=[('label',LABEL),('text',TEXT)])\n",
    "\n",
    "# Further split train_set into train/validation\n",
    "train_set, valid_set = train_set.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields of training/testing dataset:  dict_items([('label', <torchtext.data.field.Field object at 0x00000242F803AFD0>), ('text', <torchtext.data.field.Field object at 0x00000242F8A5F080>)])\n",
      "Length of training dataset:  17500\n",
      "Length of validation dataset:  7500\n",
      "Length of test dataset:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Fields of training/testing dataset: \", train_set.fields.items())\n",
    "print(\"Length of training dataset: \", len(train_set))\n",
    "print(\"Length of validation dataset: \", len(valid_set))\n",
    "print(\"Length of test dataset: \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': ['many', 'people', 'judge', 'it', 'as', 'a', 'fan', 'service', 'film', 'because', 'a', 'lot', 'of', 'super', 'star', 'starring', 'in', 'this', 'movie', 'gackt', 'hyde', 'and', 'wang', 'lee', 'hom', 'is', 'very', 'famous', 'singer', 'in', 'japan', 'but', 'don', 't', 'judge', 'it', 'before', 'you', 'watch', 'is', 'what', 'i', 'say', 'gackt', 'and', 'staff', 'are', 'very', 'serious', 'when', 'made', 'this', 'film', 'and', 'they', 'worked', 'so', 'hard', 'it', 's', 'a', 'good', 'film', 'with', 'a', 'touchy', 'story', 'inside', 'several', 'scenes', 'can', 'be', 'so', 'fun', 'and', 'some', 'others', 'are', 'so', 'sad', 'they', 'made', 'it', 'so', 'good', 'until', 'i', 'can', 't', 'stop', 'watching', 'this', 'all', 'over', 'again', 'the', 'story', 'has', 'written', 'pretty', 'well', 'but', 'i', 'admit', 'that', 'their', 'act', 'are', 'little', 'disappointing', 'this', 'is', 'especially', 'for', 'hyde', 'because', 'his', 'skill', 'of', 'acting', 'is', 'under', 'from', 'the', 'other', 'and', 'it', 'is', 'weird', 'to', 'hear', 'the', 'way', 'when', 'he', 'speaks', 'with', 'other', 'language', 'except', 'his', 'native', 'language', 'japan', 'but', 'it', 's', 'comprehensibility', 'because', 'this', 'is', 'their', 'first', 'time', 'to', 'act', 'in', 'the', 'movie', 'i', 'think', 'gackt', 'trying', 'to', 'show', 'us', 'about', 'how', 'someone', 'can', 'be', 'so', 'weak', 'when', 'they', 'lose', 'the', 'most', 'important', 'person', 'in', 'their', 'life', 'when', 'toshi', 'was', 'killed', 'when', 'sho', 'asked', 'kei', 'to', 'turn', 'yi', 'che', 'to', 'being', 'vampire', 'like', 'him', 'because', 'he', 'won', 't', 'let', 'her', 'die', 'when', 'sho', 's', 'brother', 'died', 'kei', 'shoot', 'son', 'die', 'and', 'the', 'best', 'and', 'beautiful', 'scene', 'is', 'when', 'sho', 'pass', 'away', 'even', 'i', 'told', 'that', 'hyde', 's', 'skill', 'is', 'still', 'weird', 'but', 'i', 'give', 'him', 'two', 'thumps', 'up', 'at', 'that', 'scene', 'there', 's', 'a', 'time', 'where', 'the', 'plot', 'goes', 'too', 'fast', 'like', 'they', 'didn', 't', 'tell', 'the', 'reason', 'why', 'son', 'can', 'join', 'the', 'local', 'mafia', 'and', 'being', 'sho', 's', 'enemy', 'because', 'they', 'are', 'a', 'good', 'friend', 'at', 'the', 'past', 'and', 'also', 'son', 'is', 'sho', 's', 'brother', 'in', 'law', 'whatever', 'i', 'love', 'this', 'movie', 'very', 'much', 'this', 'is', 'an', 'action', 'movie', 'with', 'a', 'touching', 'beautiful', 'story']}\n"
     ]
    }
   ],
   "source": [
    "# Checking a training data\n",
    "print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator\n",
    "data_batch_size = 128\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train_set, valid_set, test_set), batch_size=data_batch_size, \n",
    "                                                   sort_key=lambda x: len(x.text), device=device, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102661, 50])\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "LABEL.build_vocab(train_set)\n",
    "TEXT.build_vocab(train_set, valid_set, test_set, vectors=vec)\n",
    "\n",
    "# Check the vocabulary shape\n",
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102661"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '1', 'text': ['many', 'people', 'judge', 'it', 'as', 'a', 'fan', 'service', 'film', 'because', 'a', 'lot', 'of', 'super', 'star', 'starring', 'in', 'this', 'movie', 'gackt', 'hyde', 'and', 'wang', 'lee', 'hom', 'is', 'very', 'famous', 'singer', 'in', 'japan', 'but', 'don', 't', 'judge', 'it', 'before', 'you', 'watch', 'is', 'what', 'i', 'say', 'gackt', 'and', 'staff', 'are', 'very', 'serious', 'when', 'made', 'this', 'film', 'and', 'they', 'worked', 'so', 'hard', 'it', 's', 'a', 'good', 'film', 'with', 'a', 'touchy', 'story', 'inside', 'several', 'scenes', 'can', 'be', 'so', 'fun', 'and', 'some', 'others', 'are', 'so', 'sad', 'they', 'made', 'it', 'so', 'good', 'until', 'i', 'can', 't', 'stop', 'watching', 'this', 'all', 'over', 'again', 'the', 'story', 'has', 'written', 'pretty', 'well', 'but', 'i', 'admit', 'that', 'their', 'act', 'are', 'little', 'disappointing', 'this', 'is', 'especially', 'for', 'hyde', 'because', 'his', 'skill', 'of', 'acting', 'is', 'under', 'from', 'the', 'other', 'and', 'it', 'is', 'weird', 'to', 'hear', 'the', 'way', 'when', 'he', 'speaks', 'with', 'other', 'language', 'except', 'his', 'native', 'language', 'japan', 'but', 'it', 's', 'comprehensibility', 'because', 'this', 'is', 'their', 'first', 'time', 'to', 'act', 'in', 'the', 'movie', 'i', 'think', 'gackt', 'trying', 'to', 'show', 'us', 'about', 'how', 'someone', 'can', 'be', 'so', 'weak', 'when', 'they', 'lose', 'the', 'most', 'important', 'person', 'in', 'their', 'life', 'when', 'toshi', 'was', 'killed', 'when', 'sho', 'asked', 'kei', 'to', 'turn', 'yi', 'che', 'to', 'being', 'vampire', 'like', 'him', 'because', 'he', 'won', 't', 'let', 'her', 'die', 'when', 'sho', 's', 'brother', 'died', 'kei', 'shoot', 'son', 'die', 'and', 'the', 'best', 'and', 'beautiful', 'scene', 'is', 'when', 'sho', 'pass', 'away', 'even', 'i', 'told', 'that', 'hyde', 's', 'skill', 'is', 'still', 'weird', 'but', 'i', 'give', 'him', 'two', 'thumps', 'up', 'at', 'that', 'scene', 'there', 's', 'a', 'time', 'where', 'the', 'plot', 'goes', 'too', 'fast', 'like', 'they', 'didn', 't', 'tell', 'the', 'reason', 'why', 'son', 'can', 'join', 'the', 'local', 'mafia', 'and', 'being', 'sho', 's', 'enemy', 'because', 'they', 'are', 'a', 'good', 'friend', 'at', 'the', 'past', 'and', 'also', 'son', 'is', 'sho', 's', 'brother', 'in', 'law', 'whatever', 'i', 'love', 'this', 'movie', 'very', 'much', 'this', 'is', 'an', 'action', 'movie', 'with', 'a', 'touching', 'beautiful', 'story']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 831])\n"
     ]
    }
   ],
   "source": [
    "# Check the sample batch\n",
    "sample_train_batch = next(iter(train_iter))\n",
    "\n",
    "print(sample_train_batch.text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4315, -2.6598,  0.4538, -0.4569,  1.1227,  0.2325, -0.2207,  2.3917,\n",
       "        -2.2225,  1.6792, -0.8594,  1.1664,  0.8578,  0.0004, -1.2418, -3.3718,\n",
       "         0.9664,  1.0759,  5.0070,  1.2052,  1.0962,  2.0620,  1.5243,  1.4128,\n",
       "        -2.3810,  3.3441, -1.6822, -0.2576, -1.1957,  1.3121,  2.7869,  3.1460,\n",
       "         1.5586,  0.4723,  1.2427,  0.6243, -1.7932, -3.9185,  2.2868, -2.0155,\n",
       "         3.8783, -2.0760, -1.8423, -0.2244,  3.4017, -2.4928, -2.1677,  0.8344,\n",
       "         3.0949, -3.1904])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tensor for a sample input word\n",
    "TEXT.vocab.vectors[TEXT.vocab.stoi['a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 667700), ('and', 324294), ('a', 322886), ('of', 289379), ('to', 268071), ('is', 211046), ('it', 190675), ('in', 186718), ('i', 175431), ('this', 150909)]\n"
     ]
    }
   ],
   "source": [
    "# Check the top 10 frequent words in vocabulary\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = train_set[0]\n",
    "# ex.text, ex.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights, vocab_size, embedding_length=50, hidden_size=128, output_size=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = vocab_size\n",
    "        self.embedding_dim  = embedding_length\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.output_dim = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(weights)\n",
    "        self.rnn = nn.RNN(input_size=self.embedding_dim, hidden_size=self.hidden_dim)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def init_hidden(self, bs):\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.zeros(1, bs, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, bs, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, input_word):\n",
    "        # input_word -> batch_size * sent_length = 128* 784\n",
    "        # hidden -> num_layer * batch_size * hidden_dim = 1*128*128\n",
    "        batch_size = input_word.shape[0] # 784\n",
    "        h_0 = self.init_hidden(batch_size) #1*784*128\n",
    "        \n",
    "        embedded = self.embedding(input_word)   # embedded -> batch_size * sent_length * embedding_dim = 128*784*50\n",
    "        \n",
    "        embedded = embedded.permute(1,0,2)      # embedded -> sent_length * batch_size * embedding_dim = 784*128*50\n",
    "        \n",
    "        rnn_out, hidden = self.rnn(embedded, h_0)    # rnn_out -> sent_length * batch_size * hidden_dim\n",
    "                                                # hidden -> num_layer * batch_size * hidden_dim\n",
    "        output = self.fc(hidden[-1])          # output -> batch_size * output_dim\n",
    "                                    \n",
    "        return output    # output -> batch_size * output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_2(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 2 = (pos, neg)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "#         self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)\n",
    "        self.rnn = nn.RNN(embedding_length, hidden_size, num_layers=1, bidirectional=False)\n",
    "        self.label = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, batch_size, self.hidden_size))    \n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
    "        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output of the linear layer containing logits for pos & neg class which receives its input as the final_hidden_state of RNN.\n",
    "        logits.size() = (batch_size, output_size)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        input = self.word_embeddings(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        \n",
    "        h_0 = self.init_hidden(input_sentences.shape[0])\n",
    "        \n",
    "        output, h_n = self.rnn(input, h_0)\n",
    "        # h_n.size() = (4, batch_size, hidden_size)\n",
    "        h_n = h_n.permute(1, 0, 2) # h_n.size() = (batch_size, 4, hidden_size)\n",
    "        h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "        # h_n.size() = (batch_size, 4*hidden_size)\n",
    "        logits = self.label(h_n) # logits.size() = (batch_size, output_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embedding_length = vec.vectors.shape\n",
    "hidden_dim = 256\n",
    "output_dim = len(LABEL.vocab)\n",
    "\n",
    "model = RNN(weights=vec.vectors, hidden_size=hidden_dim,output_size=output_dim, vocab_size=vocab_size, embedding_length=embedding_length)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = []\n",
    "y_predicted = []\n",
    "total_loss_train = 0\n",
    "\n",
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "def train_model(model=model, train_iter=train_iter, epoch=0):\n",
    "    model.train()\n",
    "    \n",
    "    # After epoch\n",
    "    y_gt = []\n",
    "    y_predicted = []\n",
    "    steps = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for index, (X,y) in enumerate(train_iter):\n",
    "        target = X[0]\n",
    "        target = target.view(-1)\n",
    "        input_word = X[1]\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "            input_word = input_word.cuda()\n",
    "\n",
    "#         print(input_word.shape)\n",
    "        # Forward pass\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(input_word)\n",
    "#         print(output.shape)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_gradient(model, 1e-1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_idx = torch.max(output, dim=1)[1]\n",
    "\n",
    "        y_gt += list(target.cpu().data.numpy())\n",
    "        y_predicted += list(pred_idx.cpu().data.numpy())\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        acc = accuracy_score(y_gt, y_predicted)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 40 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {index+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "    train_acc = accuracy_score(y_gt, y_predicted)\n",
    "    train_loss = total_loss_train/len(train_iter)\n",
    "    \n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model=model, train_iter=val_iter, epoch=0):\n",
    "    model.eval()\n",
    "    \n",
    "    # After epoch\n",
    "    y_gt = []\n",
    "    y_predicted = []\n",
    "    steps = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for index, (X,y) in enumerate(train_iter):\n",
    "        target = X[0]\n",
    "        target = target.view(-1)\n",
    "        input_word = X[1]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "            input_word = input_word.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_word)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        pred_idx = torch.max(output, dim=1)[1]\n",
    "\n",
    "        y_gt += list(target.cpu().data.numpy())\n",
    "        y_predicted += list(pred_idx.cpu().data.numpy())\n",
    "\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "        acc = accuracy_score(y_gt, y_predicted)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 40 == 0:\n",
    "            print (f'Epoch: {epoch+1}, Idx: {index+1}, Validation Loss: {loss.item():.4f}, Validation Accuracy: {acc.item(): .2f}%')\n",
    "\n",
    "    train_acc = accuracy_score(y_gt, y_predicted)\n",
    "    train_loss = total_loss_train/len(val_iter)\n",
    "    \n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 40, Training Loss: 0.6946, Training Accuracy:  0.51%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    train_acc, train_loss = train_model(model, train_iter, epoch)\n",
    "    val_acc, val_loss = eval_model(model, val_iter, epoch)\n",
    "    \n",
    "    print (f'Epoch: {epoch+1}, Training Accuracy: {train_acc:.4f}%, Training Loss: {train_loss:.4f}, Validation Accuracy: {val_acc:.4f}%, Validation Loss: {val_loss:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
