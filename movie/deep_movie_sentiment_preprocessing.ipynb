{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 11/30/2018\n",
    "    Description \n",
    "        - Usage of Word Embedding\n",
    "        - Deep Learning\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Read all files into one corpus file\n",
    "    Break down each sentence into new line\n",
    "    Remove punctuation except selective punctuation\n",
    "    Lowercase\n",
    "    Replace numbers with <num>\n",
    "    Do not remove stop words\n",
    "    Remove '.'\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT RUN THIS SECTION TWICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    DO NOT RUN\n",
    "'''\n",
    "in_path = './data/aclImdb/data/'\n",
    "out_file = './data/raw_corpus.txt'\n",
    "\n",
    "# Create a corpus and remove html break \n",
    "with open(out_file, 'w', encoding='utf8') as out_f:\n",
    "    for root, dirs, files in os.walk(in_path, topdown=True):\n",
    "        if(len(files) > 0):\n",
    "            for each_file in files:\n",
    "                file_path = os.path.join(root, each_file)\n",
    "                content = open(file_path, encoding='utf-8-sig').read()\n",
    "                out_f.write(content)\n",
    "\n",
    "out_f.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = './data/raw_corpus.txt'\n",
    "in_f = open(out_file, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# https://stackoverflow.com/a/31505798/4595807\n",
    "'''\n",
    "    Split reviews into sentences with special conditions\n",
    "'''\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "numbers = \"([0-9])\"\n",
    "symbols = '[,\\\\\\/@#$&%\\-\\'\\(\\):;=\\+*\\^\\~\\[\\]\\\"\\|]'\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    \n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(numbers,\" <num>\",text)\n",
    "    \n",
    "    \n",
    "\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\"<stop>\")\n",
    "    text = text.replace(\"?\",\"<stop>\")\n",
    "    text = text.replace(\"!\",\"<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    text = text.replace(\"<br />\",\"\")\n",
    "    \n",
    "    text = re.sub(symbols,\"\",text)\n",
    "    text = text.lower()\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 1]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = split_into_sentences(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once again mr. costner has dragged out a movie for far longer than necessary', 'aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters']\n"
     ]
    }
   ],
   "source": [
    "print(sent[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenize each sentence\n",
    "'''\n",
    "import nltk\n",
    "total_sents = []\n",
    "\n",
    "for each in sent:\n",
    "    total_sents.append(each.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['once',\n",
       "  'again',\n",
       "  'mr.',\n",
       "  'costner',\n",
       "  'has',\n",
       "  'dragged',\n",
       "  'out',\n",
       "  'a',\n",
       "  'movie',\n",
       "  'for',\n",
       "  'far',\n",
       "  'longer',\n",
       "  'than',\n",
       "  'necessary'],\n",
       " ['aside',\n",
       "  'from',\n",
       "  'the',\n",
       "  'terrific',\n",
       "  'sea',\n",
       "  'rescue',\n",
       "  'sequences',\n",
       "  'of',\n",
       "  'which',\n",
       "  'there',\n",
       "  'are',\n",
       "  'very',\n",
       "  'few',\n",
       "  'i',\n",
       "  'just',\n",
       "  'did',\n",
       "  'not',\n",
       "  'care',\n",
       "  'about',\n",
       "  'any',\n",
       "  'of',\n",
       "  'the',\n",
       "  'characters']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating word2vec embeddings\n",
    "'''\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"./data/word2vec.model\")\n",
    "\n",
    "model = Word2Vec(total_sents, size=50, window=5, min_count=1, workers=4)\n",
    "model.wv.save_word2vec_format('./data/word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
