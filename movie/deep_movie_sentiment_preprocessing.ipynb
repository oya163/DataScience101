{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 11/30/2018\n",
    "    Description \n",
    "        - Usage of Word Embedding\n",
    "        - Deep Learning\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Read all files into one corpus file\n",
    "    Break down each sentence into new line\n",
    "    Remove punctuation except selective punctuation\n",
    "    Lowercase\n",
    "    Replace numbers with <num>\n",
    "    Do not remove stop words\n",
    "    Remove '.'\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT RUN THIS SECTION TWICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    DO NOT RUN\n",
    "'''\n",
    "in_path = './data/aclImdb/data/'\n",
    "out_file = './data/raw_corpus.txt'\n",
    "\n",
    "table = dict.fromkeys(i for i in range(sys.maxunicode) \n",
    "                        if un.category(chr(i)).startswith(('P','N','S','Cf','Cn','Cc'))\n",
    "                        and i != 45 and i != 46)\n",
    "\n",
    "# Create a corpus and remove html break \n",
    "with open(out_file, 'w', encoding='utf8') as out_f:\n",
    "    for root, dirs, files in os.walk(in_path, topdown=True):\n",
    "        if(len(files) > 0):\n",
    "            for each_file in files:\n",
    "                file_path = os.path.join(root, each_file)\n",
    "                fp = open(file_path, encoding='utf-8-sig').read().replace(\"<br />\",\"\")\n",
    "                \n",
    "                fp = fp.translate(table)\n",
    "                \n",
    "                fp = re.sub(r\"(?<!\\w)[-]|[-](?!\\w)\",'',fp)\n",
    "                \n",
    "                out_f.write(fp)\n",
    "\n",
    "out_f.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = './data/raw_corpus.txt'\n",
    "in_f = open(out_file, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__load', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_unload', 'subdir', 'unicode_repr']\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    " \n",
    "print(dir(gutenberg))\n",
    "print(gutenberg.fileids())\n",
    " \n",
    "text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    text += gutenberg.raw(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. James Ph.D.', 'someone told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Mr. James Ph.D. someone told me Dr. Brown is not available today. I will try tomorrow.\"\n",
    "print(tokenizer.tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tit', 'gent', 'deci', 'mur', \"'dr\", 'o.r', 'br', 'calp', 'len', 'oct', 'ophe', 'qu', 'cly', 'syw', 'xx', 'xxx', 'p.s', 'cass', 'j', 'var', 'ro', 'mss', 'n.e', 'cin', 'a.s', 'ara', 'treb', 'macb', 'amb', 'ant', 'pol', 'brut', 'guil', 'gen', 'malc', 'p.h', 'cop', 'cob', 'm', 'ave', 'cyn', 'w', 'm.d', 'vol', 'ment', 'pind', \"'mr\", 'jun', 'talb', 'n', 'k.c', 'ult', 'finsb', 'moo', 's.w.f', 'ed', 'g.k', 'ang', 'hora', 'sw', 'fla', 'clo', 'luc', 'cas', 'ros', 'boa', 'por', 'polon', 'b', 'osr', 'lep', 'g', 'v.e', 'wm', 'k', 'vat', 'm.p', 'u', 'feb', 'l10', 'p', 'esq', 'mes', 'ser', 'ami', 'lbs', 'dard', 'hag', 'rosin', 'fred', 'cath', 'cai', 'gho', 'fran', 'etc', 'poe', 'bap', 'rev', 'ple', 'macd', 'trans', \"'w\", 'prima', 'phil', 'dec', 'i.e', 'laer', 'mal', 'doct', 'messa', 'p.m', 'cassi', 'sey', 'c', 'burs', 'clit', 'mt', '1.ple', 'mac', 'ely', 'dut', 'bru', 'cic', 'hon', 'y.sey', 'f', 'banq', 'ibid', 'u.s', 'foh', 'stra', 'a.d', 'hec', 'dag', 'ger', 'octa'}\n",
      "{'break_decision': False,\n",
      " 'collocation': True,\n",
      " 'period_index': 2,\n",
      " 'reason': 'known collocation (both words)',\n",
      " 'text': 'Mr. James',\n",
      " 'type1': 'mr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'james',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'BEG-UC', 'UNK-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 14,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Ph.D. someone',\n",
      " 'type1': 'ph.d.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'someone',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-LC', 'BEG-UC', 'MID-LC'},\n",
      " 'type2_ortho_heuristic': False}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 34,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Dr. Brown',\n",
      " 'type1': 'dr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'brown',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-LC', 'MID-LC', 'UNK-UC', 'BEG-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 64,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'today. I',\n",
      " 'type1': 'today.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'i',\n",
      " 'type2_is_sent_starter': True,\n",
      " 'type2_ortho_contexts': {'BEG-LC',\n",
      "                          'BEG-UC',\n",
      "                          'MID-LC',\n",
      "                          'MID-UC',\n",
      "                          'UNK-LC',\n",
      "                          'UNK-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ['Mr. James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n",
    "# View the learned abbreviations\n",
    "print(tokenizer._params.abbrev_types)\n",
    "# set([...])\n",
    "# Here's how to debug every split decision\n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr. James Ph.D.', 'someone told me Dr. Brown is not available today.', 'I will try tomorrow.']\n",
      "{'break_decision': False,\n",
      " 'collocation': True,\n",
      " 'period_index': 2,\n",
      " 'reason': 'known collocation (both words)',\n",
      " 'text': 'Mr. James',\n",
      " 'type1': 'mr.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'james',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'BEG-UC', 'UNK-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 14,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Ph.D. someone',\n",
      " 'type1': 'ph.d.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'someone',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-LC', 'BEG-UC', 'MID-LC'},\n",
      " 'type2_ortho_heuristic': False}\n",
      "{'break_decision': None,\n",
      " 'collocation': False,\n",
      " 'period_index': 34,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'Dr. Brown',\n",
      " 'type1': 'dr.',\n",
      " 'type1_in_abbrs': True,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'brown',\n",
      " 'type2_is_sent_starter': False,\n",
      " 'type2_ortho_contexts': {'UNK-LC', 'MID-LC', 'UNK-UC', 'BEG-UC', 'MID-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n",
      "{'break_decision': True,\n",
      " 'collocation': False,\n",
      " 'period_index': 64,\n",
      " 'reason': 'default decision',\n",
      " 'text': 'today. I',\n",
      " 'type1': 'today.',\n",
      " 'type1_in_abbrs': False,\n",
      " 'type1_is_initial': False,\n",
      " 'type2': 'i',\n",
      " 'type2_is_sent_starter': True,\n",
      " 'type2_ortho_contexts': {'BEG-LC',\n",
      "                          'BEG-UC',\n",
      "                          'MID-LC',\n",
      "                          'MID-UC',\n",
      "                          'UNK-LC',\n",
      "                          'UNK-UC'},\n",
      " 'type2_ortho_heuristic': 'unknown'}\n"
     ]
    }
   ],
   "source": [
    "tokenizer._params.abbrev_types.add('d')\n",
    " \n",
    "print(tokenizer.tokenize(sentences))\n",
    "# ['Mr. James told me Dr. Brown is not available today.', 'I will try tomorrow.']\n",
    " \n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "#     print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# https://stackoverflow.com/a/31505798/4595807\n",
    "'''\n",
    "    Split reviews into sentences with special conditions\n",
    "'''\n",
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "numbers = \"([0-9])\"\n",
    "# symbols = \"[,\\\\\\/@#$&%\\-'\\(\\):;=\\+*\\^\\~\\[\\]\\\"\\|_`><¿”’\\´½‘£ß]\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    \n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(numbers,\" <num> \",text)\n",
    "#     text = re.sub(symbols,\" \",text)\n",
    "    \n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\"<stop>\")\n",
    "    text = text.replace(\"?\",\"<stop>\")\n",
    "    text = text.replace(\"!\",\"<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    text = text.replace(\"<br />\",\"\")\n",
    "    \n",
    "    text = text.lower()\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 1]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = split_into_sentences(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at least lucas didnt recycle his old footage\n"
     ]
    }
   ],
   "source": [
    "print(sent[85669])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nce again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrifi\n"
     ]
    }
   ],
   "source": [
    "print(in_f[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Tokenize each sentence\n",
    "'''\n",
    "import nltk\n",
    "total_sents = []\n",
    "\n",
    "for each in sent:\n",
    "    total_sents.append(each.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'not',\n",
       " 'historically',\n",
       " 'accurateactually',\n",
       " 'only',\n",
       " 'soldiers',\n",
       " 'of',\n",
       " 'th',\n",
       " 'rota',\n",
       " 'were',\n",
       " 'killed',\n",
       " 'there',\n",
       " 'and',\n",
       " 'yes',\n",
       " 'it',\n",
       " 'has',\n",
       " 'some',\n",
       " 'mistakes',\n",
       " 'and',\n",
       " 'exaggerationbended',\n",
       " 'machine',\n",
       " 'gun',\n",
       " 'come',\n",
       " 'on',\n",
       " 'or',\n",
       " 'the',\n",
       " 'that',\n",
       " 'history',\n",
       " 'lesson',\n",
       " 'about',\n",
       " 'how',\n",
       " 'afghanistan',\n",
       " 'was',\n",
       " 'never',\n",
       " 'conquered',\n",
       " 'by',\n",
       " 'anyone',\n",
       " 'educated',\n",
       " 'russian',\n",
       " 'officer',\n",
       " 'would',\n",
       " 'know',\n",
       " 'history',\n",
       " 'much',\n",
       " 'better',\n",
       " 'than',\n",
       " 'that',\n",
       " 'take',\n",
       " 'for',\n",
       " 'example',\n",
       " 'british',\n",
       " 'campaign',\n",
       " 'in',\n",
       " 'afghanistan']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Creating word2vec embeddings\n",
    "'''\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"./data/word2vec.model\")\n",
    "\n",
    "model = Word2Vec(total_sents, size=50, window=5, min_count=1, workers=4)\n",
    "model.wv.save_word2vec_format('./data/word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
