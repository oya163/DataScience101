{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 11/05/2018\n",
    "    Description \n",
    "        - Practising NLP\n",
    "        - Text preprocessing\n",
    "        - Analyzing various ML algorithms\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "# Checking the raw file\n",
    "file = open('./data/aclImdb/train/pos/0_9.txt')\n",
    "print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train/text files\n",
    "pd_train = pd.read_csv('./data/aclImdb/train.csv')\n",
    "pd_test = pd.read_csv('./data/aclImdb/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25001, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ashishashishvirtualbox filehomeashishconfiglib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i get tired of my  and  year old daughters con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this movie is outrageous funny ribald sophisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i have to start saying it has been a long time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>if the very thought of arthur askey twists you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               data\n",
       "0      0  ashishashishvirtualbox filehomeashishconfiglib...\n",
       "1      1  i get tired of my  and  year old daughters con...\n",
       "2      1  this movie is outrageous funny ribald sophisti...\n",
       "3      1  i have to start saying it has been a long time...\n",
       "4      1  if the very thought of arthur askey twists you..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train =  31408174\n",
      "Number of tokens in test =  30660597\n"
     ]
    }
   ],
   "source": [
    "train_token_count = 0\n",
    "test_token_count = 0\n",
    "\n",
    "for each_row in pd_train['data']:\n",
    "    train_token_count += len(each_row)\n",
    "    \n",
    "for each_row in pd_test['data']:\n",
    "    test_token_count += len(each_row)\n",
    "\n",
    "print(\"Number of tokens in train = \", train_token_count)\n",
    "print(\"Number of tokens in test = \", test_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Number of tokens in each sentence in train =  1256.2767089316428\n",
      "Avg Number of tokens in each sentence in test =  1226.42388\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg Number of tokens in each sentence in train = \", train_token_count/pd_train.shape[0])\n",
    "print(\"Avg Number of tokens in each sentence in test = \", test_token_count/pd_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Putting all data into a list\n",
    "train_tokens_list = pd_train.data.tolist()\n",
    "test_tokens_list = pd_test.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acted'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if the token_list can be easily tokenized\n",
    "train_tokens_list[10].split()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'up'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if the token_list can be easily tokenized\n",
    "test_tokens_list[10].split()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for each in train_tokens_list:\n",
    "    for i in each.split():\n",
    "        tokens.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 4815),\n",
       " (('this', 'movie', 'is'), 2476),\n",
       " (('of', 'the', 'film'), 2363),\n",
       " (('a', 'lot', 'of'), 2255),\n",
       " (('this', 'is', 'a'), 2140),\n",
       " (('of', 'the', 'movie'), 2039),\n",
       " (('some', 'of', 'the'), 1855),\n",
       " (('is', 'one', 'of'), 1746),\n",
       " (('the', 'film', 'is'), 1686),\n",
       " (('this', 'film', 'is'), 1669)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_trigrams = ngrams(tokens, 3)\n",
    "\n",
    "Counter(text_trigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('is', 'one', 'of', 'the'), 1099),\n",
       " (('the', 'rest', 'of', 'the'), 1006),\n",
       " (('one', 'of', 'the', 'most'), 853),\n",
       " (('the', 'end', 'of', 'the'), 737),\n",
       " (('one', 'of', 'the', 'best'), 713),\n",
       " (('i', 'have', 'ever', 'seen'), 578),\n",
       " (('this', 'is', 'one', 'of'), 567),\n",
       " (('at', 'the', 'same', 'time'), 475),\n",
       " (('one', 'of', 'the', 'worst'), 454),\n",
       " (('at', 'the', 'end', 'of'), 431)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_trigrams = ngrams(tokens, 4)\n",
    "\n",
    "Counter(text_trigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138189"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_unigram = ngrams(tokens, 1)\n",
    "\n",
    "c = Counter(text_unigram)\n",
    "\n",
    "# Get total count of unique words\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and test list\n",
    "full_text = train_tokens_list + test_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = Counter(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 412 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f86514ddbe0>>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer.fit(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694176"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a a', 'a aa', 'a abandoned', 'a about', 'a absent', 'a absolute', 'a absolutely', 'a absolutley', 'a absurd']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "pd_train = pd_train.sample(frac=1).reset_index(drop=True)\n",
    "pd_test = pd_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ghost town starts as kate barrett catherine hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fantastic documentary of  this early th centur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this was a superb episode one of the best of b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i voted excellent for how well the acting was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ive long wanted to see this film being a fan o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               data\n",
       "0      2  ghost town starts as kate barrett catherine hi...\n",
       "1      1  fantastic documentary of  this early th centur...\n",
       "2      1  this was a superb episode one of the best of b...\n",
       "3      1  i voted excellent for how well the acting was ...\n",
       "4      1  ive long wanted to see this film being a fan o..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data after shuffled\n",
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our input train/test data\n",
    "train_vectorized = vectorizer.transform(pd_train.data)\n",
    "test_vectorized = vectorizer.transform(pd_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25001, 2694176)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of transformed vector\n",
    "print(train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with random new review\n",
    "review = [\"This movie is fine\", \"Nobody watched the movie\"]\n",
    "\n",
    "# Print the prediction of new reviews\n",
    "def print_prediction(review, est):\n",
    "    print(\"*****EVALUATE REVIEW*******\")\n",
    "    review_vectorized = vectorizer.transform(review)\n",
    "    y_pred = est.predict(review_vectorized)\n",
    "    for each_review, each_y_pred in zip(review, y_pred):\n",
    "        if each_y_pred != 1:\n",
    "            pred_review = \"Negative\"\n",
    "        else:\n",
    "            pred_review = \"Positive\"\n",
    "        print(\"{0}: {1}\".format(each_review, pred_review))\n",
    "        \n",
    "        \n",
    "def print_acc(est, acc):\n",
    "    print(\"Accuracy of {0}: {1:2.3%}\".format(est, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING SECTION STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oyashi/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 120 ms, total: 29 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Logistic Regression\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)\n",
    "\n",
    "ovr.fit(train_vectorized, pd_train.label.values)\n",
    "ovr_acc = ovr.score(test_vectorized, pd_test.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs Rest Logisitic Regression accuracy =  0.88312\n"
     ]
    }
   ],
   "source": [
    "print(\"One vs Rest Logisitic Regression accuracy = \", ovr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is fine: Negative\n",
      "The movie is not fine: Positive\n"
     ]
    }
   ],
   "source": [
    "print_prediction(review, ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Cross Validation Logistic Regression\n",
    "'''\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cv_ovr = LogisticRegression()\n",
    "\n",
    "cv_ovr = cross_validate(cv_ovr, train_vectorized, pd_train.label.values, scoring='accuracy', n_jobs=-1, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8768 0.8872 0.8908 0.8928 0.894  0.886  0.8932 0.8904 0.8828 0.894 ]\n"
     ]
    }
   ],
   "source": [
    "print(cv_ovr['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean (test) accuracy 88.88%, std 0.54.\n"
     ]
    }
   ],
   "source": [
    "print('Cross-validation mean (test) accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(cv_ovr['test_score']) * 100, np.std(cv_ovr['test_score']) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ovr = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [1, 10, 100, 1000] }\n",
    "\n",
    "ovr_grid = GridSearchCV(ovr, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "ovr_grid_fit = ovr_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression CV: 90.384%\n"
     ]
    }
   ],
   "source": [
    "ovr_grid_score = ovr_grid_fit.score(test_vectorized, pd_test.label.values)\n",
    "print_acc(acc=ovr_grid_score, est='Logistic Regression CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was good: Positive\n",
      "There was normal acting: Negative\n"
     ]
    }
   ],
   "source": [
    "# Testing with random new review\n",
    "print_prediction(review, ovr_grid_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy 0.6966\n",
      "Wall time: 1h 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    SVM\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "my_svm = SVC()\n",
    "\n",
    "my_svm.fit(train_vectorized, pd_train.label.values)\n",
    "\n",
    "svm_acc = my_svm.score(test_vectorized, pd_test.label.values)\n",
    "print('SVM accuracy', svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is fine: Positive\n",
      "The movie is not fine: Positive\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    SVM accuracy 0.6966\n",
    "    Wall time: 1h 2min 31s\n",
    "'''\n",
    "\n",
    "# Evaluate the trained algorithm with new data\n",
    "print_prediction(review, my_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Grid Search CV SVM\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_grid = SVC()\n",
    "\n",
    "param_grid = {'C': [1, 10, 100, 1000], 'gamma': [1e-3, 1e-4]}\n",
    "\n",
    "svm_grid = GridSearchCV(svm_grid, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "svm_grid_fit = svm_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MultinomialNB: 86.336%\n",
      "Movie was good: Negative\n",
      "There was normal acting: Negative\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Naive Bayes MultinomialNB\n",
    "'''\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB \n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "nb_score = nb_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('MultinomialNB', nb_score)\n",
    "\n",
    "review = ['Movie was good', 'There was normal acting']\n",
    "print_prediction(review, nb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MultinomialNB: 85.084%\n",
      "Movie was good: Negative\n",
      "There was normal acting: Negative\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Naive Bayes BernoulliNB\n",
    "'''\n",
    "\n",
    "nb_bern_clf = BernoulliNB()\n",
    "nb_bern_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "nb_bern_score = nb_bern_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('MultinomialNB', nb_bern_score)\n",
    "\n",
    "print_prediction(review, nb_bern_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN Classifier: 68.012%\n",
      "Movie was good: Positive\n",
      "There was normal acting: Negative\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    KNN Classifier\n",
    "'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "knn_score = knn_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('KNN Classifier', knn_score)\n",
    "\n",
    "print_prediction(review, knn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oyashi/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 1s, sys: 1min 8s, total: 25min 10s\n",
      "Wall time: 24min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 24.3min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Grid Search CV kNN Classifier\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf_grid = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 2, 3, 5, 10]}\n",
    "\n",
    "knn_clf_grid = GridSearchCV(knn_clf_grid, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "knn_grid_fit = knn_clf_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GridSearch kNN: 74.669%\n",
      "*****EVALUATE REVIEW*******\n",
      "He watched a movie: Negative\n",
      "He was awarded for that kind of acting: Positive\n"
     ]
    }
   ],
   "source": [
    "print_acc(acc=knn_grid_fit.best_score_, est='GridSearch kNN')\n",
    "\n",
    "review = ['He watched a movie', 'He was awarded for that kind of acting']\n",
    "print_prediction(review, knn_grid_fit.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron Classifier: 87.980%\n",
      "CPU times: user 2.56 s, sys: 4 ms, total: 2.56 s\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Perceptron\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "percept = Perceptron(tol=1e-3, random_state=0)\n",
    "percept.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "percept_score = percept.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('Perceptron Classifier', percept_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****EVALUATE REVIEW*******\n",
      "He watched a good movie: Positive\n",
      "nothing is good: Positive\n"
     ]
    }
   ],
   "source": [
    "review = ['He watched a good movie', 'nothing is good']\n",
    "print_prediction(review, percept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oyashi/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed: 16.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 39s, sys: 2.45 s, total: 17min 41s\n",
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Grid Search CV Perceptron\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "percept_grid = Perceptron()\n",
    "\n",
    "param_grid = {'alpha':[0.0001, 0.001, 0.01, 0.1],'max_iter': [10, 50, 100, 200]}\n",
    "\n",
    "percept_grid = GridSearchCV(percept_grid, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "percept_grid_fit = percept_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GridSearch CV Perceptron Classifier: 88.984%\n",
      "*****EVALUATE REVIEW*******\n",
      "He watched a good movie: Positive\n",
      "The hall seats were empty: Negative\n"
     ]
    }
   ],
   "source": [
    "print_acc('GridSearch CV Perceptron Classifier', percept_grid_fit.best_score_)\n",
    "\n",
    "review = ['He watched a good movie', 'The hall seats were empty']\n",
    "print_prediction(review, percept_grid_fit.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Ensemble - Adaboost\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - Ensemble\n",
    "        - Adaboost\n",
    "        - GradientBoostingClassifier\n",
    "        - VotingClassifier\n",
    "    - PCA\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
