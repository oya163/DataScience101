{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Author - Oyesh Mann Singh\n",
    "    Date - 11/05/2018\n",
    "    Description \n",
    "        - Practising NLP\n",
    "        - Text preprocessing\n",
    "        - Analyzing various ML algorithms\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import unicodecsv as csv\n",
    "import unicodedata as un\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "# Checking the raw file\n",
    "file = open('./data/aclImdb/train/pos/0_9.txt')\n",
    "print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train/text files\n",
    "pd_train = pd.read_csv('./data/aclImdb/train.csv')\n",
    "pd_test = pd.read_csv('./data/aclImdb/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>airport   starts as a brand new luxury  plane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this film lacked something i couldn t put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sorry everyone    i know this is supposed to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               data\n",
       "0      1  story of a man who has unnatural feelings for ...\n",
       "1      1  airport   starts as a brand new luxury  plane ...\n",
       "2      1  this film lacked something i couldn t put my f...\n",
       "3      1  sorry everyone    i know this is supposed to b...\n",
       "4      1  when i was little my parents took me along to ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in train =  32430009\n",
      "Number of tokens in test =  31661792\n"
     ]
    }
   ],
   "source": [
    "train_token_count = 0\n",
    "test_token_count = 0\n",
    "\n",
    "for each_row in pd_train['data']:\n",
    "    train_token_count += len(each_row)\n",
    "    \n",
    "for each_row in pd_test['data']:\n",
    "    test_token_count += len(each_row)\n",
    "\n",
    "print(\"Number of tokens in train = \", train_token_count)\n",
    "print(\"Number of tokens in test = \", test_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Number of tokens in each sentence in train =  1297.20036\n",
      "Avg Number of tokens in each sentence in test =  1266.47168\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg Number of tokens in each sentence in train = \", train_token_count/pd_train.shape[0])\n",
    "print(\"Avg Number of tokens in each sentence in test = \", test_token_count/pd_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Putting all data into a list\n",
    "train_tokens_list = pd_train.data.tolist()\n",
    "test_tokens_list = pd_test.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pant'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if the token_list can be easily tokenized\n",
    "train_tokens_list[10].split()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gets'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if the token_list can be easily tokenized\n",
    "test_tokens_list[10].split()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for each in train_tokens_list:\n",
    "    for i in each.split():\n",
    "        tokens.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 4941),\n",
       " (('i', 'don', 't'), 2705),\n",
       " (('this', 'movie', 'is'), 2674),\n",
       " (('of', 'the', 'film'), 2611),\n",
       " (('this', 'is', 'a'), 2379),\n",
       " (('it', 's', 'a'), 2376),\n",
       " (('a', 'lot', 'of'), 2276),\n",
       " (('of', 'the', 'movie'), 2182),\n",
       " (('some', 'of', 'the'), 1909),\n",
       " (('the', 'film', 'is'), 1872)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_trigrams = ngrams(tokens, 3)\n",
    "\n",
    "Counter(text_trigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('is', 'one', 'of', 'the'), 1100),\n",
       " (('the', 'rest', 'of', 'the'), 1060),\n",
       " (('one', 'of', 'the', 'most'), 872),\n",
       " (('one', 'of', 'the', 'best'), 749),\n",
       " (('the', 'end', 'of', 'the'), 746),\n",
       " (('i', 'don', 't', 'know'), 703),\n",
       " (('this', 'is', 'one', 'of'), 623),\n",
       " (('i', 'have', 'ever', 'seen'), 616),\n",
       " (('i', 'don', 't', 'think'), 527),\n",
       " (('i', 've', 'ever', 'seen'), 521)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_trigrams = ngrams(tokens, 4)\n",
    "\n",
    "Counter(text_trigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74870"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the most commong trigrams\n",
    "text_unigram = ngrams(tokens, 1)\n",
    "\n",
    "c = Counter(text_unigram)\n",
    "\n",
    "# Get total count of unique words\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and test list\n",
    "full_text = train_tokens_list + test_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = Counter(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x0000026F390F8BA8>>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342118"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a a', 'a aa', 'a abandoned', 'a about', 'a absent', 'a absolute', 'a absolutely', 'a absolutley', 'a absurd']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "pd_train = pd_train.sample(frac=1).reset_index(drop=True)\n",
    "pd_test = pd_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>gene tierney and dana andrews  who were both s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>one of the best  amitabh comeback  movies i li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>star rating        the works      just misses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>its about time that gunga din is released on d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>trapped  buried alive brings us to a resort th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               data\n",
       "0      2  gene tierney and dana andrews  who were both s...\n",
       "1      2  one of the best  amitabh comeback  movies i li...\n",
       "2      1  star rating        the works      just misses ...\n",
       "3      2  its about time that gunga din is released on d...\n",
       "4      2  trapped  buried alive brings us to a resort th..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data after shuffled\n",
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our input train/test data\n",
    "train_vectorized = vectorizer.transform(pd_train.data)\n",
    "test_vectorized = vectorizer.transform(pd_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2342118)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of transformed vector\n",
    "print(train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with random new review\n",
    "review = [\"This movie is fine\", \"The movie is not fine\"]\n",
    "\n",
    "# Print the prediction of new reviews\n",
    "def print_prediction(review, est):\n",
    "    review_vectorized = vectorizer.transform(review)\n",
    "    y_pred = est.predict(review_vectorized)\n",
    "    for each_review, each_y_pred in zip(review, y_pred):\n",
    "        if each_y_pred == 1:\n",
    "            pred_review = \"Negative\"\n",
    "        else:\n",
    "            pred_review = \"Positive\"\n",
    "        print(\"{0}: {1}\".format(each_review, pred_review))\n",
    "        \n",
    "        \n",
    "def print_score(est, acc):\n",
    "    print(\"Accuracy of {0}: {1:2.3%}\".format(est, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING SECTION STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Logistic Regression\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)\n",
    "\n",
    "ovr.fit(train_vectorized, pd_train.label.values)\n",
    "ovr_acc = ovr.score(test_vectorized, pd_test.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs Rest Logisitic Regression accuracy =  0.88616\n"
     ]
    }
   ],
   "source": [
    "print(\"One vs Rest Logisitic Regression accuracy = \", ovr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is fine: Positive\n",
      "The movie is not fine: Negative\n"
     ]
    }
   ],
   "source": [
    "print_prediction(review, ovr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Cross Validation Logistic Regression\n",
    "'''\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cv_ovr = LogisticRegression()\n",
    "\n",
    "cv_ovr = cross_validate(cv_ovr, train_vectorized, pd_train.label.values, scoring='accuracy', n_jobs=-1, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8768 0.8872 0.8908 0.8928 0.894  0.886  0.8932 0.8904 0.8828 0.894 ]\n"
     ]
    }
   ],
   "source": [
    "print(cv_ovr['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean (test) accuracy 88.88%, std 0.54.\n"
     ]
    }
   ],
   "source": [
    "print('Cross-validation mean (test) accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(cv_ovr['test_score']) * 100, np.std(cv_ovr['test_score']) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ovr = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [1, 10, 100, 1000] }\n",
    "\n",
    "ovr_grid = GridSearchCV(ovr, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "ovr_grid_fit = ovr_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression CV: 90.384%\n"
     ]
    }
   ],
   "source": [
    "ovr_grid_score = ovr_grid_fit.score(test_vectorized, pd_test.label.values)\n",
    "print_acc(acc=ovr_grid_score, est='Logistic Regression CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie was good: Positive\n",
      "There was normal acting: Negative\n"
     ]
    }
   ],
   "source": [
    "# Testing with random new review\n",
    "print_prediction(review, ovr_grid_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy 0.6966\n",
      "Wall time: 1h 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    SVM\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "my_svm = SVC()\n",
    "\n",
    "my_svm.fit(train_vectorized, pd_train.label.values)\n",
    "\n",
    "svm_acc = my_svm.score(test_vectorized, pd_test.label.values)\n",
    "print('SVM accuracy', svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is fine: Positive\n",
      "The movie is not fine: Positive\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    SVM accuracy 0.6966\n",
    "    Wall time: 1h 2min 31s\n",
    "'''\n",
    "\n",
    "# Evaluate the trained algorithm with new data\n",
    "print_prediction(review, my_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Grid Search CV SVM\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_grid = SVC()\n",
    "\n",
    "param_grid = {'C': [1, 10, 100, 1000], 'gamma': [1e-3, 1e-4]}\n",
    "\n",
    "svm_grid = GridSearchCV(svm_grid, param_grid=param_grid, cv=5, refit=True, verbose=1)\n",
    "\n",
    "svm_grid_fit = svm_grid.fit(train_vectorized, pd_train.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MultinomialNB: 86.336%\n",
      "Movie was good: Negative\n",
      "There was normal acting: Negative\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Naive Bayes MultinomialNB\n",
    "'''\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB \n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "nb_score = nb_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('MultinomialNB', nb_score)\n",
    "\n",
    "review = ['Movie was good', 'There was normal acting']\n",
    "print_prediction(review, nb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MultinomialNB: 85.084%\n",
      "Movie was good: Negative\n",
      "There was normal acting: Negative\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    Naive Bayes BernoulliNB\n",
    "'''\n",
    "\n",
    "nb_bern_clf = BernoulliNB()\n",
    "nb_bern_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "nb_bern_score = nb_bern_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('MultinomialNB', nb_bern_score)\n",
    "\n",
    "print_prediction(review, nb_bern_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN Classifier: 68.012%\n",
      "Movie was good: Positive\n",
      "There was normal acting: Negative\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "    KNN Classifier\n",
    "'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X=train_vectorized, y=pd_train.label.values)\n",
    "knn_score = knn_clf.score(test_vectorized, pd_test.label.values)\n",
    "\n",
    "print_acc('KNN Classifier', knn_score)\n",
    "\n",
    "print_prediction(review, knn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - Perceptron (NN)\n",
    "    - Ensemble\n",
    "    - PCA\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
